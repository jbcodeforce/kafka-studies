{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Apache Kafka Studies","text":"<p>This repository regroups a set of personal studies and quick summaries on Kafka. Most of the curated contents are  defined in the Kafka overview article, and the producer and consumer blog.</p>"},{"location":"#kafka-local","title":"Kafka local","text":"<p>The docker compose in this repo, starts one zookeeper and one Kafka broker locally using last Strimzi release, one Apicurio for schema registry and Kafdrop for UI.</p> <p>In the docker compose the Kafka defines two listeners, for internal communication using the DNS name <code>kafka</code> on port 29092 and one listener for external communication on port 9092.</p> <p></p> <p>A quarkus app, for example, running with <code>quarkus:dev</code> will connect to localhost:9092. But a container in the same network needs to access kafka via its hostname. </p> <p>To start kafkacat and kafkacat doc to access sample consumer - producer</p> <pre><code>docker run -it --network=host edenhill/kafkacat -b kafka:9092 -L\n</code></pre>"},{"location":"#security-summary","title":"Security summary","text":"<p>For deeper dive on security administration see Confluent article and Apache Kafka product documentation.</p> <p>The settings that are important to consider:</p> <ul> <li>security.protocol See how the listeners are configured in Kafka. The valid values are:</li> </ul> <pre><code>PLAINTEXT (using PLAINTEXT transport layer &amp; no authentication - default value).\nSSL (using SSL transport layer &amp; certificate-based authentication)\nSASL_PLAINTEXT (using PLAINTEXT transport layer &amp; SASL-based authentication)\nSASL_SSL (using SSL transport layer &amp; SASL-based authentication)\n</code></pre> <p>In Strimzi the following yaml defines the different listeners type and port: <code>tls</code> boolean is for the traffic encryption, while <code>authentication.type</code> will define the matching security protocol.</p> <pre><code>listeners:\n      - name: plain\n        port: 9092\n        type: internal\n        tls: false\n      - name: tls\n        port: 9093\n        type: internal\n        tls: true\n        authentication:\n          type: tls\n      - name: external\n        type: route\n        port: 9094\n        tls: true \n        authentication:\n          type: scram-sha-512\n</code></pre> <ul> <li><code>ssl.truststore.location</code> and <code>ssl.truststore.password</code>: when doing TLS encryption we need to provide our Kafka clients with the location of a trusted Certificate Authority-based certificate. This file is often provided by the Kafka administrator and is generally unique to the specific Kafka cluster deployment. The certificate is in JKS format for JVM languages and PEM/ P12 for nodejs or Python.</li> </ul> <p>To extract a PEM-based certificate from a JKS-based truststore, we can use the following command: </p> <pre><code>keytool -exportcert -keypass {truststore-password} -keystore {provided-kafka-truststore.jks} -rfc -file {desired-kafka-cert-output.pem}\n</code></pre> <ul> <li>sasl.mechanism for authentication protocol used. Possible values are:</li> </ul> <pre><code>PLAIN (cleartext passwords, although they will be encrypted across the wire per security.protocol settings above)\nSCRAM-SHA-512 (modern Salted Challenge Response Authentication Mechanism)\nGSSAPI (Kerberos-supported authentication and the default if not specified otherwise)\n</code></pre> <ul> <li>for java based app, the <code>sasl.jaas.config</code> strings are:</li> </ul> <pre><code>sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{USERNAME}\" password=\"{PASSWORD}\";\nsasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username=\"{USERNAME}\" password=\"{PASSWORD}\";\n</code></pre> <p>For external connection to Strimzi cluster use the following, where USERNAME is a scram-user</p> <pre><code>bootstrap.servers={kafka-cluster-name}-kafka-bootstrap-{namespace}.{kubernetes-cluster-fully-qualified-domain-name}:443\nsecurity.protocol=SASL_SSL\nsasl.mechanism=SCRAM-SHA-512\nsasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"{USERNAME}\" password=\"{PASSWORD}\";\nssl.truststore.location={/provided/to/you/by/the/kafka/administrator}\nssl.truststore.password={__provided_to_you_by_the_kafka_administrator__}\n</code></pre> <p>To get the user password get the user secret (oc or kubectl CLIs):</p> <pre><code>oc get secret scram-user -o jsonpath='{.data.admin_password}' | base64 --decode &amp;&amp; echo \"\"\n</code></pre> <p>To get the Bootstrap URL use: </p> <pre><code>expost K_CLUSTER_NAME=mycluster\nexport BOOTSTRAP=\"$(oc get route ${K_CLUSTER_NAME}-kafka-bootstrap -o jsonpath='{.spec.host}'):443\"\n</code></pre> <p>The <code>sasl.jaas.config</code> can come from an environment variable inside of a secret, but in fact it is already predefined in the scram user in Strimzi:</p> <pre><code>oc get secret my-user -o json | jq -r '.data[\"sasl.jaas.config\"]' | base64 -d -\n</code></pre> <ul> <li>For internal communication, with PLAIN the setting is:</li> </ul> <pre><code>bootstrap.servers={kafka-cluster-name}-kafka-bootstrap.{namespace}.svc.cluster.local:9093\nsecurity.protocol = SASL_PLAINTEXT (these clients do not require SSL-based encryption as they are local to the cluster)\nsasl.mechanism = PLAIN\nsasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{USERNAME}\" password=\"{PASSWORD}\";\n</code></pre> <ul> <li>For internal authentication with mutual TLS the settings:</li> </ul> <pre><code>security.protocol=SSL \n</code></pre> <p>Remember that if the application does not run in the same namespace as the kafka cluster then copy the secrets to the namespace with something like:</p> <pre><code>if [[ -z $(oc get secret ${TLS_USER} 2&gt; /dev/null) ]]\nthen\n   # As the project is personal to the user, we can keep a generic name for the secret\n   oc get secret ${TLS_USER} -n ${KAFKA_NS} -o json | jq -r '.metadata.name=\"tls-user\"' | jq -r '.metadata.namespace=\"'${YOUR_PROJECT_NAME}'\"' | oc apply -f -\nfi\n\nif [[ -z $(oc get secret ${SCRAM_USER} 2&gt; /dev/null) ]]\nthen\n    # As the project is personal to the user, we can keep a generic name for the secret\n    oc get secret ${SCRAM_USER} -n ${KAFKA_NS} -o json |  jq -r '.metadata.name=\"scram-user\"' | jq -r '.metadata.namespace=\"'${YOUR_PROJECT_NAME}'\"' | oc apply -f -\nfi\n</code></pre>"},{"location":"#using-kafdrop","title":"Using Kafdrop","text":"<p>For Kafdrop configuration see the <code>kafka.properties</code> file and <code>startKafDrop.sh</code> in the scripts folder.</p> <p>To get the user password:</p> <pre><code># Get certificate to put in truststore\noc get secret kafka-cluster-ca-cert -o jsonpath='{.data.ca\\.p12}' | base64 --decode &gt;./certs/kafka.p12\n# Get certificate password\noc get secret kafka-cluster-ca-cert -o jsonpath='{.data.ca\\.password}' | base64 --decode \n# Get user passwor from the jaas config\noc get secret scram-user -o jsonpath='{.data.sasl\\.jaas\\.config}' | base64 --decode &amp;&amp; echo\n</code></pre> <p>Those results are set in the properties:</p> <pre><code>ssl.truststore.password=\nusername=\"scram-user\" password=\"\";\n</code></pre>"},{"location":"#using-apicurio","title":"Using Apicurio","text":"<p>Once Apicurio is started, define a schema in json and upload it to the api: http://apicurio:8080/api. Here is an example of command:</p> <p>Schema: <pre><code>{   \n    \"namespace\": \"acme.vaccine.orderoptimizer\",\n    \"doc\": \"Avro data schema for Reefer events\",\n    \"type\":\"record\",\n    \"name\":\"Reefer\",\n    \"fields\":[\n            {\n                \"name\": \"reefer_id\",\n                \"type\": \"string\",\n                \"doc\": \"Reefer container ID\"\n            },\n            {\n                \"name\": \"status\",\n                \"type\": \"string\",\n                \"doc\": \"Reefer Container ID status. Could be an Enum\"\n            },\n            {\n                \"name\": \"location\",\n                \"type\": \"string\",\n                \"doc\": \"Reefer container location\"\n            },\n            {\n                \"name\": \"date_available\",\n                \"type\": \"string\",\n                \"doc\": \"Date when the inventory will be available\"\n            }\n     ]\n}\n</code></pre></p> <p>Upload it to schema registry</p> <pre><code>curl -X POST -H \"Content-type: application/json; artifactType=AVRO\" \\\n   -H \"X-Registry-ArtifactId: vaccine.reefers-value\" \\\n   --data @${scriptDir}/../data/avro/schemas/reefer.avsc http://localhost:8080/api/artifacts\n</code></pre>"},{"location":"#this-repository-includes","title":"This repository includes","text":"<p>Outside of my personal notes, some folders include running app:</p> <ul> <li>python-kafka for simple reusable code for event consumer and producer with python.</li> <li>Kafka Vertx starter code from the event streams team, within one app to test a deployed event stream deployment</li> <li>vertx consumer and producer as separate quarkus apps.</li> </ul>"},{"location":"#source-of-information","title":"Source of information","text":"<ul> <li>My notes on event-driven architecture</li> <li> <p>Another introduction from Confluent, one of the main contributors of the open source.</p> </li> <li> <p>Planning IBM event-streams installation</p> </li> <li>Using Kafka Connect to connect to enterprise MQ systems - Andrew Schofield</li> <li>Does Apache Kafka do ACID transactions? - Andrew Schofield</li> <li>Spark and Kafka with direct stream, and persistence considerations and best practices</li> <li>Example in scala for processing Tweets with Kafka Streams</li> </ul>"},{"location":"#strimzi","title":"Strimzi","text":"<ul> <li>This is an example of configuration to do a TLS connection on external route to a Strimzi deployed on k8s. </li> </ul> <pre><code># get environment variables from configmap\nquarkus.openshift.env.configmaps=vaccine-order-ms-cm\n# use docker compose kafka\n%dev.kafka.bootstrap.servers=kafka:9092\n</code></pre> <p>with matching config map</p> <p><code>yaml apiVersion: v1 kind: ConfigMap metadata:   name: vaccine-order-ms-cm data:   KAFKA_BOOTSTRAP_SERVERS: eda-dev-kafka-bootstrap-eventstreams.gse-eda-2021-1-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443   KAFKA_SSL_PROTOCOL: TLSv1.2   KAFKA_USER: scram   KAFKA_SSL_TRUSTSTORE_LOCATION: /deployments/certs/server/ca.p12   KAFKA_SSL_TRUSTSTORE_TYPE: PKCS12   SHIPMENT_PLAN_TOPIC: vaccine_shipment_plans   KAFKA_SASL_MECHANISM: SCRAM-SHA-512   KAFKA_SECURITY_PROTOCOL: SASL_SSL</code></p>"},{"location":"#kafka-with-quarkus","title":"Kafka with Quarkus","text":"<p>Here is a template code for quarkus based Kafka consumer: quarkus-event-driven-consumer-microservice-template.</p> <p>Read this interesting guide with Quarkus and kafka streaming: Quarkus using Kafka Streams, which is implemented in the quarkus-reactive-msg producer, aggregator folders.</p> <p>To generate the starting code for the producer we use the quarkus maven plugin with kafka extension: <code>mvn io.quarkus:quarkus-maven-plugin:1.12.2.Final:create -DprojectGroupId=jbcodeforce.kafka.study -DprojectArtifactId=producer -Dextensions=\"kafka\"</code></p> <p>for the aggregator:</p> <p><code>mvn io.quarkus:quarkus-maven-plugin:1.12.2.Final:create -DprojectGroupId=jbcodeforce.kafka.study -DprojectArtifactId=aggregator -Dextensions=\"kafka-streams,resteasy-jsonb\"</code></p> <p>Interesting how to generate reference value to a topic with microprofile reactive messaging. <code>stations</code> is a hash:</p> <pre><code>    @Outgoing(\"weather-stations\")                               \n    public Flowable&lt;KafkaRecord&lt;Integer, String&gt;&gt; weatherStations() {\n        List&lt;KafkaRecord&lt;Integer, String&gt;&gt; stationsAsJson = stations.stream()\n            .map(s -&gt; KafkaRecord.of(\n                    s.id,\n                    \"{ \\\"id\\\" : \" + s.id +\n                    \", \\\"name\\\" : \\\"\" + s.name + \"\\\" }\"))\n            .collect(Collectors.toList());\n\n        return Flowable.fromIterable(stationsAsJson);\n    };\n</code></pre> <p>Channels are mapped to Kafka topics using the Quarkus configuration file <code>application.properties</code>.</p> <p>To build and run:</p> <pre><code># under producer folder\ndocker build -f src/main/docker/Dockerfile.jvm -t quarkstream/producer-jvm .\n# under aggregator folder\ndocker build -f src/main/docker/Dockerfile.jvm -t quarkstream/aggregator-jvm .\n# Run under quarkus-reactive-msg\ndocker-compose up\n# Run kafkacat\ndocker run --tty --rm -i --network kafkanet debezium/tooling:1.0\n$ kafkacat -b kafka1:9092 -C -o beginning -q -t temperatures-aggregated\n</code></pre>"},{"location":"FAQ/","title":"Frequently asked questions","text":"<p>Moved to eda faq</p>"},{"location":"FAQ/#how-internal-and-external-listerner-work","title":"How internal and external listerner work","text":"<p>See this article to understand thee listener configuration. Here is a config to be used in docker container:</p> <pre><code> KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092\n KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT\n KAFKA_LISTENERS: EXTERNAL://0.0.0.0:9092,INTERNAL://kafka:29092\n KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL\n</code></pre>"},{"location":"FAQ/#some-error-and-solution","title":"Some error and solution","text":""},{"location":"FAQ/#classcastexception-ibmedademoappinfrastructureeventsorderevent-incompatible-with-javalangstring","title":"ClassCastException: ibm.eda.demo.app.infrastructure.events.OrderEvent incompatible with java.lang.String","text":"<p>We need to have a special serializer for the bean. Quarkus has some serialization in Json.</p>"},{"location":"FAQ/#other-faqs","title":"Other FAQs","text":"<p>IBM Event streams on Cloud FAQ </p> <p>FAQ from Confluent</p>"},{"location":"kstreams/","title":"Kafka Streams examples","text":"<p>In this article we are presenting how to use the Kafka Streams API combined with Kafka event sourcing to implement different interesting use cases.</p> <p>The use cases are implemented inside the kstreams-play java project as unit tests.  Some of the domain classes are defined in the <code>src/java</code> folder. Streams topology are done in the unit test but could also be part of a service class to be used as an example running with kafka.</p>"},{"location":"kstreams/#lab-1-mask-credit-card-number","title":"Lab 1: mask credit card number","text":"<p>Using a simple kstream to change data on the fly, like for example encrypt a credit card number. Test is EncryptCreditCardTest. This is the first test to use the TopologyTestDriver class to run business logic outside of kafka. The class uses org.apache.kafka.common.serialization.Serdes and String serdesm and a JSON serdes for the domain class Purchase.</p> <p>The other interesting approach is to use domain object with builder class and DSL to define the model:</p> <pre><code>Purchase.builder(p).maskCreditCard().build()\n</code></pre>"},{"location":"kstreams/#joining-3-streams-with-reference-data-to-build-a-document","title":"Joining 3 streams with reference data to build a document","text":"<p>This is a simple example of joining 3 sources of kafka streams to build a merged document, with some reference data loaded from a topic:</p> <ul> <li>The shipment status is a reference table and loaded inside a kafka topic: shipment-status</li> <li>The order includes items and customer id reference</li> <li>Customer is about the customer profile</li> <li>Products is about products inventory. </li> <li>The outcome is an order report document that merge most of the attributes of the 3 streams.</li> </ul>"},{"location":"kstreams/#refarch-container-the-container-inventory-management-implementation","title":"Refarch container the container inventory management implementation","text":"<p>The component can be represented in the figure below:</p> <p></p> <p>For getting started with Kafka Streams API read this tutorial.</p> <p>The container topics includes all the events about a refrigerator container life cycle. The application is Java based and deployed in Liberty packaged into a docker image deployable on Kubernetes. The service exposes some RESTful APIs to get a container by ID. No CUD operations are defined as all those operations are done via events. The Streams implementation keeps data in Ktable.</p> <p>As a java based microservice we have two approaches to implement the service: springboot and microprofile. Knowing we will deploy on kubernetes cluster with Istio we will have a lot of the resiliency and scalability addressed for us. Microprofile adds a lot of nice capabilities like SSL, open API, JAXRS, injection, metrics... we can leverage for this application. Microprofile is supported by Open Liberty as well as many servers.</p> <p>The Apache Kafka Streams API is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It simplifies the implementation of the stateless or stateful event processing to transform and enrich data. It supports time windowing processing.</p> <p>We encourage to do this Streams tutorial. </p> <p>The features we want to illustrate in this implementation, using KStreams are:</p> <ul> <li>Listen to ContainerAddedToInventory event from the <code>containers</code> topic and maintains a stateful table of containers. </li> <li>Listen to OrderCreated event from <code>orders</code> and assign a container from the inventory based on the pickup location and the container location and its characteristics.</li> <li>Implemented as JAXRS application deployed on Liberty and packaged with dockerfile.</li> <li>Deploy to kubernetes or run with docker-compose</li> </ul>"},{"location":"kstreams/#some-useful-kafka-streams-apis","title":"Some useful Kafka streams APIs","text":"<p>The stream configuration looks similar to the Kafka consumer and producer configuration. </p> <pre><code>    props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"container-streams\");\n    props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");    \n    props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n    props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n</code></pre> <p>The StreamsConfig is a specific configuration for Streams app.  </p> <p>One of the interesting class is the <code>KStream</code> to manage a stream of structured events. Kstreams represents unbounded collection of immutable events.</p> <p>Two classes are supporting the order and container processing:</p> <ul> <li>ContainerInventoryView</li> <li>ContainerOrderAssignment</li> </ul> <p>We are using the Streams DSL APIs to do the processing. Here is an example of terminal stream to print what is coming to the <code>orders</code> topic:</p> <pre><code>   final StreamsBuilder builder = new StreamsBuilder();\n    builder.stream(\"orders\")\n        .foreach((key,value) -&gt; {\n                    Order order = parser.fromJson((String)value, OrderEvent.class).getPayload();\n                    // TODO do something to the order\n                    System.out.println(\"received order \" + key + \" \" + value);\n                });\n\n    final Topology topology = builder.build();\n    final KafkaStreams streams = new KafkaStreams(topology, props);\n    streams.start();\n</code></pre> <p>We want now to implement the container inventory. We want to support the following events:</p> <ul> <li>ContainerAddedToInventory, ContainerRemovedFromInventory</li> <li>ContainerAtLocation</li> <li>ContainerOnMaintenance, ContainerOffMaintenance,</li> <li>ContainerAssignedToOrder, ContainerReleasedFromOrder</li> <li>ContainerGoodLoaded, ContainerGoodUnLoaded</li> <li>ContainerOnShip, ContainerOffShip</li> <li>ContainerOnTruck, ContainerOffTruck</li> </ul> <p>We want the container event to keep a timestamp, a version, a type, and a payload representing the data describing a Reefer container. The Key is the containerID. The java class for the container event is here.</p> <p>Using a TDD approach we will start by the tests to implement the solution.</p> <p>For more information on the Streams DSL API, keep this page close to you. </p>"},{"location":"kstreams/#test-driven-development","title":"Test Driven Development","text":"<p>We want to document two major test suites. One for building the internal view of the container inventory, the other to support the container to order assignment.</p>"},{"location":"kstreams/#container-inventory","title":"Container inventory","text":"<p>When the service receives a ContainerAdded event it needs to add it to the table and be able to retreive it by ID</p> <ol> <li>To support the Get By ID we are adding a Service class with the operation exposed as RESTful resource using JAXRS annotations. We already described this approach in the fleetms project.</li> </ol> <p>To test a stream application without Kafka backbone there is a test utility available here. The settings are simple: get the properties, define the serialisation of the key and value of the event to get from kafka, define the stream process flow, named topology, send the input and get the output.</p> <p>The test TestContainerInventory is illustrating how to use the TopologyTestDriver.</p> <pre><code>    Properties props = ApplicationConfig.getStreamsProperties(\"test\");\n    props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummy:1234\");\n    TopologyTestDriver testDriver = new TopologyTestDriver(\n                buildProcessFlow(), props);\n\n    ConsumerRecordFactory&lt;String, String&gt; factory = new ConsumerRecordFactory&lt;String, String&gt;(\"containers\",\n                new StringSerializer(), new StringSerializer());\n    ConsumerRecord&lt;byte[],byte[]&gt; record = factory.create(\"containers\",ce.getContainerID(), parser.toJson(ce));\n\n    testDriver.pipeInput(record);\n</code></pre> <p>We are using the String default serialization for the key and the ContainerEvent, and use Gson to serialize and deserialize the json.</p> <p>So the test is to prepare a ContainerEvent with type = \"ContainerAdded\" and then get the payload, persist it in the table and access to the table via the concept of <code>store</code> and validate the data.</p> <p>Below is the access to the store and compare the expected results</p> <pre><code>    KeyValueStore&lt;String, String&gt; store = testDriver.getKeyValueStore(\"queryable-container-store\");\n    String containerStrg = store.get(ce.getContainerID());\n    Assert.assertNotNull(containerStrg);\n    Assert.assertTrue(containerStrg.contains(ce.getContainerID()));\n    Assert.assertTrue(containerStrg.contains(\"atDock\"));\n</code></pre> <p>Now the tricky part is in the Stream process flow. The idea is to process the ContainerEvent as streams (of String) and extract the payload (a Container), then generate the Container in a new stream, group by the key and then save to a table. We separate the code in a function so e can move it into the real application after.</p> <pre><code>    public  Topology buildProcessFlow() {\n        final StreamsBuilder builder = new StreamsBuilder();\n       // containerEvent is a string, map values help to change the type and data of the inpit values\n        builder.stream(CONTAINERS_TOPIC).mapValues((containerEvent) -&gt; {\n                 // the container payload is of interest to keep in table\n                 Container c = jsonParser.fromJson((String)containerEvent, ContainerEvent.class).getPayload();\n                 return jsonParser.toJson(c);\n             }).groupByKey()  // the keys are kept so we can group by key to prepare for the tabl\n                .reduce((container,container1) -&gt; {\n                    System.out.println(\"received container \" + container1 );\n                    return container1;\n                },\n                     Materialized.as(\"queryable-container-store\"));\n        return builder.build();\n    }\n</code></pre> <p>The trick is to use the <code>reduce()</code> function that get the container and save it to the store that we can specify.</p> <p>The unit test runs successfully with the command: <code>mvn -Dtest=TestContainerInventory test</code>.</p> <p>This logic can be integrated in a View class. So we can refactor the test and add new class (see ContainerInventoryView class) to move the logic into the applciation. From a design point of view this class is a DAO. Now that we are not using the Testing tool, we need the real streams.</p> <p>In class ContainerInventoryView:</p> <pre><code>   private KafkaStreams streams;\n   // ..\n    Properties props = ApplicationConfig.getStreamsProperties(\"container-streams\");\n    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n    streams = new KafkaStreams(buildProcessFlow(), props);\n    try {\n        streams.cleanUp(); \n        streams.start();\n    } catch (Throwable e) {\n        System.exit(1);\n    }\n</code></pre> <p>As illustrated above, the streams API is a continuous running Thread, so it needs to be started only one time. We will address scaling separatly.  So we isolate the DAO as a Singleton, and start it when the deployed application starts, via a ServletContextListener. </p> <pre><code>public class EventLoop implements ServletContextListener{\n\n    @Override\n    public void contextInitialized(ServletContextEvent sce) {\n        // Initialize the Container consumer\n        ContainerInventoryView cView = (ContainerInventoryView)ContainerInventoryView.instance();\n        cView.start();\n    }\n</code></pre> <p>Now we can add the getById operation, package as a war, deploy it to Liberty. </p>"},{"location":"kstreams/#container-to-order-assignment","title":"Container to Order Assignment","text":"<p>The business logic we want to implement is to get an order with the source pickup city, the type of product, the quantity and the expected pickup date, manage the internal list of containers and search for a container located close to the pickup city from the order.</p> <p>The test to validate this logic is under <code>kstreams/src/test/java/ut/TestContainerAssignment</code>. </p> <p>The story will not be completed if we do not talk about how th application get the order. As presented in the design and order command microservice implementation, when an order is created an event is generated to the <code>orders</code> topic. So we need to add another Streams processing and start the process flow in the context listener.</p> <p></p>"},{"location":"kstreams/#run-tests","title":"Run tests","text":"<p>Recall with maven we can run all the unit tests, one test and skip integration tests.</p> <pre><code># Test a unique test\n$  mvn -Dtest=TestContainerInventory test\n# Skip all tests\nmvn install -DskipTests\n# Skip integration test\nmvn install -DskipITs\n# Run everything\nmvn install\n</code></pre>"},{"location":"kstreams/#how-streams-flows-are-resilient","title":"How streams flows are resilient?","text":"<p>Specifying the replicas factor at the topic level, with a cluster of kafka brokers, combine with transactional event produce, ensure to do not lose messages. The producer client code has the list of all the brokers to contact in case of failure and will try to connect to any broker in the list. </p>"},{"location":"kstreams/#how-to-scale","title":"How to scale?","text":""},{"location":"kstreams/#further-readings","title":"Further readings","text":"<ul> <li>Kafka Streams\u2019 Take on Watermarks and Triggers what continuous refinement with operational parameters means: </li> <li>Tutorial: Introduction to Streaming Application Development</li> <li>Multi threaded messaging</li> </ul>"},{"location":"mirrormaker/","title":"Mirror Maker 2.0 Studies","text":"<p>Mirror Maker 2.0 is the new replication feature of Kafka 2.4. In this note we are presenting different test scenarios for topic replication.</p> <ul> <li>Replicate from local cluster to Event Streams on Cloud (See detail in the scenario 1 section)</li> <li>Replicate from Strimzi 'local' Kafka cluster running on OpenShift to Event Streams on Cloud. (See detail in the scenario 2 section)</li> <li>Replicate from Event Streams on cloud being the source cluster to local Kafka cluster running on local machine (started via docker-compose) using Strimzi Kafka docker image.</li> <li>Replicate from Event Streams on premise running on Openshift being the source cluster to Event Stream on the Cloud as target cluster.</li> </ul> Environment Source Target Connect 1 Local Event Streams on Cloud Local 2 Strimzi on OCP Event Streams on Cloud OCP / Roks 3 Event Streams on Cloud Local Local 4 Event Streams on Cloud Strimzi on OCP OCP/ Roks 5 Event Streams on OCP Event Streams on Cloud OCP / Roks <p>The <code>mirror-maker-2</code> folder includes, scripts, code and configurations to support those scenarios.</p>"},{"location":"mirrormaker/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>You need to have one Event Streams service created on IBM Cloud.</li> <li>You may need to use Event Streams CLI. So follow those instructions to get it.</li> </ul> <p>The following ibmcloud CLI command presents the Event Stream cluster's metadata, like the broker list and the cluster ID:</p> <pre><code>ibmcloud es cluster\n</code></pre> <p>For other CLI commands see this summary.</p> <ul> <li>To run local cluster we use docker-compose and docker. The docker compose file to start a local 3 Kafka brokers and 2 Zookeepers cluster is in <code>mirror-maker-2/local-cluster</code> folder. This compose file uses a local docker network called <code>kafkanet</code>. The docker image used for Kafka is coming from Strimzi open source project and is for the Kafka 2.4 version.</li> <li>When the Event Streams service is created, add a service credentials and get the brokers list and api key information. We will use them in a setenv.sh script file under <code>mirror-maker-2</code> folder to define environment variables.</li> </ul>"},{"location":"mirrormaker/#general-concepts","title":"General concepts","text":"<p>As Mirror maker 2.0 is using kafka Connect framework, we recommend to review our summary in this note. </p> <p>The figure below illustrates the mirror maker internal components running within Kafka Connect.</p> <p></p> <p>In distributed mode, Mirror Maker creates the following topics to the target cluster:</p> <ul> <li>mm2-configs.source.internal: This topic will store the connector and task configurations.</li> <li>mm2-offsets.source.internal: This topic is used to store offsets for Kafka Connect.</li> <li>mm2-status.source.internal: This topic will store status updates of connectors and tasks.</li> <li>source.heartbeats</li> <li>source.checkpoints.internal</li> </ul> <p>A typical mirror maker configuration is done via property file and defines source and target clusters with their connection properties and the replication flow definitions. Here is a simple example for a local cluster to a target cluster using TLS v1.2 and Sasl authentication protocol.</p> <pre><code>clusters=source, target\nsource.bootstrap.servers=${KAFKA_SOURCE_BROKERS}\ntarget.bootstrap.servers=${KAFKA_TARGET_BROKERS}\ntarget.security.protocol=SASL_SSL\ntarget.ssl.protocol=TLSv1.2\ntarget.ssl.endpoint.identification.algorithm=https\ntarget.sasl.mechanism=PLAIN\ntarget.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=${KAFKA_TARGET_APIKEY};\n# enable and configure individual replication flows\nsource-&gt;target.enabled=true\nsource-&gt;target.topics=products\ntasks.max=10\n</code></pre> <ul> <li>White listed topics are set with the <code>source-&gt;target.topics</code> attribute of the replication flow and uses Java regular expression syntax.</li> <li>Blacklisted topics: by default the following pattern is applied:</li> </ul> <p><pre><code>blacklist = [follower\\.replication\\.throttled\\.replicas, leader\\.replication\\.throttled\\.replicas, message\\.timestamp\\.difference\\.max\\.ms, message\\.timestamp\\.type, unclean\\.leader\\.election\\.enable, min\\.insync\\.replicas]\n</code></pre> but can be also specified with the properties: <code>topics.blacklist</code>. Comma-separated lists are also supported and Java regular expression.</p> <p>Internally <code>MirrorSourceConnector</code> and <code>MirrorCheckpointConnector</code> will create multiple tasks (up to <code>tasks.max</code> property), <code>MirrorHeartbeatConnector</code> creates only one single task. <code>MirrorSourceConnector</code> will have one task per topic-partition to replicate, while <code>MirrorCheckpointConnector</code> will have one task per consumer group. The Kafka connect framework uses the coordinator API, with the <code>assign()</code> API, so there is no consumer group while fetching data from source topic. There is no call to <code>commit()</code> neither: the rebalancing occurs only when there is a new topic created that matches the whitelist pattern.</p>"},{"location":"mirrormaker/#scenario-1-from-kafka-local-as-source-to-event-streams-on-cloud-as-target","title":"Scenario 1: From Kafka local as source to Event Streams on Cloud as Target","text":"<p>The test scenario goal is to send the product definitions in the local <code>products</code> topic and then start mirror maker to see the data replicated to the <code>source.products</code> topic in Event Streams cluster.</p> <p></p> <ul> <li>Set the environment variables in <code>setenv.sh</code> script for the source broker to be your local cluster, and the target to be event streams. Be sure to also set Event Streams APIKEY:</li> </ul> <pre><code>export KAFKA_SOURCE_BROKERS=kafka1:9092,kafka2:9093,kafka3:9094\n\nexport KAFKA_TARGET_BROKERS=broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\nexport KAFKA_TARGET_APIKEY=\"&lt;password attribut in event streams credentials&gt;\"\n</code></pre> <ul> <li>It may be needed to create the topics in the target cluster. This depends if mirror maker 2.0 is able to access the AdminClient API. Normally we observed with Event streams APIKEY it is possible to create topic with AdminClient, so there is no need to do the following commands. For other configuration where Access Control policies do not authorize program to create topic dynamically, the commands performed by and admin user will create the needed topic. (the mm2 prefix is the one used by mirror maker, but the name of the topic could be defined in the mirror maker properties)</li> </ul> <pre><code>ibmcloud es topic-create -n mm2-configs.source.internal -p 1  -c cleanup.policy=compact\nibmcloud es topic-create -n mm2-offsets.source.internal -p 25 -c cleanup.policy=compact\nibmcloud es topic-create -n mm2-status.source.internal -p 5 -c cleanup.policy=compact\nibmcloud es topic-create -n source.products -p 1\nibmcloud es topic-create -n source.heartbeats -p 1 -c cleanup.policy=compact\nibmcloud es topic-create -n source.checkpoints.internal -p 1 -c cleanup.policy=compact\n</code></pre> <ul> <li>In one Terminal window, start the local cluster using <code>docker-compose</code> under the <code>mirror-maker-2/local-cluster</code> folder: <code>docker-compose up &amp;</code>. The data are persisted on the local disk in this folder.</li> <li>If this is the first time you started the source cluster, you need to create the <code>products</code> topic. Start a Kafka container to access the Kafka tools with the command:</li> </ul> <pre><code>docker run -ti -v $(pwd):/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash\n</code></pre> <p>Then in the bash shell, go to <code>/home/local-cluster</code> folder and execute the script: <code>./createProductsTopic.sh</code>. Verify topic is created with the command: <code>/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka1:9092 --list</code></p> <ul> <li>Send some products data to this topic. For that we use a docker python image. The docker file to build this image is <code>python-kafka/Dockerfile-python</code> so the command to build this image (if you change the image name be sure to use the new name in future command) is: <code>docker build -f Dockerfile-python -t jbcodeforce/python37 .</code></li> </ul> <p>Once the image is built, start the python environment with the following commands:</p> <pre><code>source ./setenv.sh\ndocker run -ti -v $(pwd):/home --rm -e KAFKA_BROKERS=$KAFKA_SOURCE_BROKERS --network kafkanet jbcodeforce/python37   bash\n</code></pre> <p>In this isolated python container bash shell do the following to send the 5 first products:</p> <pre><code>$ echo $KAFKA_BROKERS\nkafka1:9092,kafka2:9093,kafka3:9094\n$ python SendProductToKafka.py ./data/products.json\n\n[KafkaProducer] - {'bootstrap.servers': 'kafka1:9092,kafka2:9093,kafka3:9094', 'group.id': 'ProductsProducer'}\n{'product_id': 'P01', 'description': 'Carrots', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P02', 'description': 'Banana', 'target_temperature': 6, 'target_humidity_level': 0.6, 'content_type': 2}\n{'product_id': 'P03', 'description': 'Salad', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P04', 'description': 'Avocado', 'target_temperature': 6, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P05', 'description': 'Tomato', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 2}\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n</code></pre> <ul> <li>To validate the data are in the source topic we can use the kafka console consumer. Here are the basic commands:</li> </ul> <pre><code>docker run -ti -v $(pwd):/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash\n$ cd bin\n$ ./kafka-console-consumer.sh --bootstrap-server kafka1:9092 --topic products --from-beginning\n</code></pre> <ul> <li>Define the event streams cluster properties file for the Kafka tool command. Set the password attribute of the <code>jaas.config</code> to match Event Streams APIKEY. The <code>eventstream.properties</code> file looks like:</li> </ul> <pre><code>bootstrap.servers=broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\nsecurity.protocol=SASL_SSL\nssl.protocol=TLSv1.2\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=....;\n</code></pre> <ul> <li>Restart the <code>kafka-console-consumer</code> with the bootstrap URL to access to Event Streams and with the replicated topic: <code>source.products</code>. Use the previously created properties file to get authentication properties so the command looks like:</li> </ul> <pre><code>source /home/setenv.sh\n./kafka-console-consumer.sh --bootstrap-server $KAFKA_TARGET_BROKERS --consumer.config /home/eventstream.properties --topic source.products --from-beginning\n</code></pre> <ul> <li>Now we are ready to start Mirror Maker 2.0, close to the local cluster, using, yet another docker image:</li> </ul> <pre><code>docker run -ti -v $(pwd):/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash\n$ /home/local-cluster/launchMM2.sh\n</code></pre> <p>This <code>launchMM2.sh</code> script is updating a template properties file with the values of the environment variables and calls with this updated file: <code>/opt/kafka/bin/connect-mirror-maker.sh mm2.properties</code></p> <p>The trace includes a ton of messages, which displays different Kafka connect consumers and producers, workers and tasks. The logs can be found in the <code>/tmp/logs</code> folder within the docker container. The table includes some of the elements of this configuration:</p> Name Description Worker clientId=connect-2, groupId=target-mm2 Herder for target cluster topics but reading source topic Producer clientId=producer-1 Producer to taget cluster Consumer clientId=consumer-target-mm2-1, groupId=target-mm2] Subscribed to 25 partition(s): mm2-offsets.target.internal-0 to 24 Consumer clientId=consumer-target-mm2-2, groupId=target-mm2] Subscribed to 5 partition(s): mm2-status.target.internal-0 to 4 Consumer clientId=consumer-target-mm2-3, groupId=target-mm2] Subscribed to partition(s): mm2-configs.target.internal-0 Worker clientId=connect-2, groupId=target-mm2 . Starting connectors and tasks using config offset 6. This trace shows mirror maker will start to consume message from the offset 6. A previous run has already committed the offset for this client id. This illustrate a Mirror Maker restarts Starting connector MirrorHeartbeatConnector and Starting task MirrorHeartbeatConnector-0 Starting connector MirrorCheckpointConnector Starting connector MirrorSourceConnector <p>As expected, in the consumer console we can see the 5 product messages arriving to the <code>source.topics</code> after the replication complete.</p> <pre><code>{'bootstrap.servers': 'kafka1:9092,kafka2:9093,kafka3:9094', 'group.id': 'ProductsProducer'}\n  {'product_id': 'P01', 'description': 'Carrots', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n  {'product_id': 'P02', 'description': 'Banana', 'target_temperature': 6, 'target_humidity_level': 0.6, 'content_type': 2}\n  {'product_id': 'P03', 'description': 'Salad', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n  {'product_id': 'P04', 'description': 'Avocado', 'target_temperature': 6, 'target_humidity_level': 0.4, 'content_type': 1}\n  {'product_id': 'P05', 'description': 'Tomato', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 2}\n</code></pre>"},{"location":"mirrormaker/#scenario-2-run-mirror-maker-2-cluster-close-to-target-cluster","title":"Scenario 2: Run Mirror Maker 2 Cluster close to target cluster","text":"<p>This scenario is similar to the scenario 1 but Mirror Maker 2.0 now, runs within an OpenShift cluster in the same data center as Event Streams cluster, so closer to the target cluster:</p> <p></p> <p>We have created an Event Streams cluster on Washington DC data center. We have Strimzi operators deployed in Washington data center OpenShift Cluster.</p> <p>Producers are running locally on the same OpenShift cluster, where vanilla Kafka is running, or can run remotely using exposed Kafka brokers Openshift route. (The black rectangles in the figure above represent those producers.)</p> <p>What needs to be done:</p> <ul> <li>Get a OpenShift cluster in the same data center as Event Streams service: See this product introduction.</li> <li>Create a project in OpenShift, for example: <code>mirror-maker-2-to-es</code>. Remember it is mapped to a namespace in Kubernetes.</li> <li>At the minimum, to run Mirror Maker 2, we need to deploy the Strimzi Custom Resource Definitions, and the Mirror Maker 2.0 operator. See the detail in sections from the deployment note. The 0.17.0 source is in this repository, unzip and use the <code>install</code> folder with Strimzi installation instructions.</li> </ul> <p>The service account and role binding do not need to be re-installed if you did it previously.</p> <ul> <li> <p>If not done yet, create a secret for the API KEY of the Event Streams cluster: <code>oc create secret generic es-api-secret --from-literal=password=&lt;replace-with-event-streams-apikey&gt;</code></p> </li> <li> <p>As the vanilla kafka source cluster is using TLS to communicate between client and brokers, we need to create a k8s secret for a Java truststore created from the <code>ca.cert</code> of the source cluster. This certificate is also in another secret: <code>my-cluster-clients-ca-cert</code>. </p> </li> </ul> <pre><code># build a local crt file from the secret: \noc extract secret/my-cluster-clients-ca-cert --keys=ca.crt --to=- &gt; ca.crt\n# Verify the certificate:\nopenssl x509 -in ca.crt -text\n# transform it for java truststore.jks:\nkeytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt\n# create a secret from file\noc create secret generic kafka-truststore --from-file=./truststore.jks\n# Verify the created secret\noc describe secret kafka-truststore\n</code></pre> <p>Attention</p> <p>At this step, we have two options, one using the Mirror Maker Operator and get the configuration of it via a yaml file, or use properties file and a special docker image. As of 3/20/2020 we have found an issue on Strimzi 0.17-rc2 MM operator, so we are proposing to use the properties approach as documented in this separated note.</p> <ul> <li>Define source and target cluster properties in mirror maker 2.0 <code>kafka-to-es-mm2.yml</code> descriptor file. We strongly recommend to study the schema definition of this custom resource from this page. The yaml file we used is here.</li> </ul> <p>Note</p> <p>connectCluster defined the cluster alias used for Kafka Connect, it must match a cluster in the list at <code>spec.clusters</code>. The config part can match the Kafka configuration for consumer or producer, except properties starting by ssl, sasl, security, listeners, rest, bootstarp.servers which are declared at the cluster definition level. Also we have some challenges to make the connection to event streams working, as of Strimzi version 0.17 RC2, we need to add an empty <code>tls: {}</code> stanza to get connected. Also below, the declaration is using the previously defined secret for event streams API key.</p> <pre><code>  alias: \"event-streams-wdc-as-target\"\n    bootstrapServers: broker-3...\n    tls: {}\n    authentication:\n      passwordSecret:\n          secretName: es-api-secret  \n          password: password \n      username: token\n      type: plain\n</code></pre> <ul> <li>Deploy Mirror maker 2.0 within this project. </li> </ul> <pre><code>oc apply -f kafka-to-es-mm2.yaml \n</code></pre> <p>This commmand create a kubernetes deployment as illustrated below, with one pod as the replicas is set to 1. If we need to add parallel processing because of the topic to replicate has multiple partitions, or there are a lot of topics to replicate, then adding pods will help to scale horizontally. The pods are in the same consumer group, so Kafka Brokers will do the partition rebalancing among those new added consumers.</p> <p></p> <ul> <li>To validate the replication works, we will connect a consumer to the <code>source.products</code> topic on Event Streams. So we define a target cluster property file (<code>eventstreams.properties</code>) like:</li> </ul> <pre><code>bootstrap.servers=broker-3-q.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-q.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\nsecurity.protocol=SASL_SSL\nssl.protocol=TLSv1.2\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"am_...\";\n</code></pre> <ul> <li>Start a producer to send product records to the source Kafka cluster. If you have done the scenario 1, the first product definitions may be already in the target cluster, so we can send a second batch of products using a second data file:</li> </ul> <pre><code>export KAFKA_BROKERS=\"my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443\"\nexport KAFKA_CERT=\"/home/ca.crt\"\ndocker run -ti -v $(pwd):/home --rm -e KAFKA_CERT=$KAFKA_CERT -e KAFKA_BROKERS=$KAFKA_BROKERS jbcodeforce/python37   bash\npython SendProductToKafka.py ./data/products2.json\n</code></pre> <p>Note</p> <p>The python code uses the CA certificate and not the java truststore. The Kafka option is <code>ssl.ca.location</code>. If the code was done in Java then the trustore needs to be part of the docker image or mounted from a kubernetes secret into the expected file inside the container.</p> <p>As an alternate to use this external producer, we can start a producer as pod inside Openshift, and then send the product one by one:</p> <pre><code>oc run kafka-producer -ti --image=strimzi/kafka:latest-kafka-2.4.0  --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic products\nIf you don t see a command prompt, try pressing enter.\n\n&gt;{'product_id': 'P01', 'description': 'Carrots', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n&gt;{'product_id': 'P02', 'description': 'Banana', 'target_temperature': 6, 'target_humidity_level': 0.6, 'content_type': 2}\n&gt;{'product_id': 'P03', 'description': 'Salad', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n&gt;{'product_id': 'P04', 'description': 'Avocado', 'target_temperature': 6, 'target_humidity_level': 0.4, 'content_type': 1}\n&gt;{'product_id': 'P05', 'description': 'Tomato', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 2}\n</code></pre> <ul> <li>To validate the source <code>products</code> topic has records, start a consumer as pod on Openshift within the source Kafka cluster using the Strimzi/kafka image.</li> </ul> <pre><code>oc run kafka-consumer -ti --image=strimzi/kafka:latest-kafka-2.4.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic products --from-beginning\n</code></pre> <ul> <li>Finally to validate the product records are replicated to the Event Streams <code>source.products</code> we need to start a consumer connected to Event streams.</li> </ul> <pre><code>oc run kafka-consumer -ti --image=strimzi/kafka:latest-kafka-2.4.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 --consumer-property ssl.protocol=TLSv1.2 --consumer-property security.protocol=SASL_SSL --consumer-property sasl.jaas.config=\"org.apache.kafka.common.security.plain.PlainLoginModule required username=token password=am_rbb9e794mMwhE-KGPYo0hhW3h91e28OhT8IlruFe5;\" --consumer-property sasl.mechanism=PLAIN --topic source.products --from-beginning\n</code></pre>"},{"location":"mirrormaker/#scenario-3-from-event-streams-to-local-cluster","title":"Scenario 3: From Event Streams to local cluster","text":"<p>For this scenario the source is Event Streams on IBM Cloud and the target is a local server (may be on a laptop using vanilla Kafka image (Strimzi kafka 2.4 docker image) started with docker compose). This target cluster runs two zookeeper nodes, and three kafka nodes. We need 3 kafka brokers as mirror maker created topics with a replication factor set to 3.</p> <p></p> <p>This time the producer adds headers to the Records sent so we can validate headers replication. The file <code>es-cluster/es-mirror-maker.properties</code> declares the mirroring settings as below:</p> <pre><code>clusters=source, target\nsource.bootstrap.servers=broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\nsource.security.protocol=SASL_SSL\nsource.ssl.protocol=TLSv1.2\nsource.sasl.mechanism=PLAIN\nsource.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"985...\";\ntarget.bootstrap.servers=kafka1:9092,kafka2:9093,kafka3:9094\n# enable and configure individual replication flows\nsource-&gt;target.enabled=true\nsource-&gt;target.topics=orders\n</code></pre> <ul> <li> <p>Start the target cluster runnning on your laptop using:</p> <pre><code>docker-compose up\n</code></pre> </li> <li> <p>Start mirror maker2.0: </p> <p>By using a new container, start another kakfa 2.4+ docker container, connected to the  brokers via the <code>kafkanet</code> network, and mounting the configuration in the <code>/home</code>:</p> <pre><code>docker run -ti --network kafkanet -v $(pwd):/home strimzi/kafka:latest-kafka-2.4.0 bash\n</code></pre> <p>Inside this container starts mirror maker 2.0 using the script: <code>/opt/kakfa/bin/connect-mirror-maker.sh</code></p> <pre><code>/opt/kakfa/bin/connect-mirror-maker.sh /home/strimzi-mm2.properties\n</code></pre> <p>The <code>strimzi-mm2.properties</code> properties file given as argument defines the source and target clusters and the topics to replicate:</p> <pre><code>clusters=source, target\nsource.bootstrap.servers=my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443\nsource.security.protocol=SSL\nsource.ssl.truststore.password=password\nsource.ssl.truststore.location=/home/truststore.jks\ntarget.bootstrap.servers=kafka1:9092,kafka2:9093,kafka3:9094\n# enable and configure individual replication flows\nsource-&gt;target.enabled=true\nsource-&gt;target.topics=orders\n</code></pre> <p>As the source cluster is deployed on Openshift, the exposed route to access the brokers is using TLS connection. So we need the certificate and create a truststore to be used by those Java programs. All kafka tools are done in java or scala so running in a JVM, which needs truststore for keep trusted TLS certificates.  When running from a remote system to get the certificate do the following steps:</p> <ol> <li> <p>Get the host ip address from the Route resource</p> <pre><code>oc get routes my-cluster-kafka-bootstrap -o=jsonpath='{.status.ingress[0].host}{\"\\n\"}'\n</code></pre> </li> <li> <p>Get the TLS certificate from the broker</p> <pre><code>oc get secrets\noc extract secret/my-cluster-cluster-ca-cert --keys=ca.crt --to=- &gt; ca.crt\n</code></pre> </li> <li> <p>Transform the certificate fo java truststore</p> <pre><code>keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt\n</code></pre> </li> </ol> <p>For Openshift or Kubernetes deployment, the mirror maker descriptor needs to declare the TLS stamza:</p> <pre><code>mirrors:\n- sourceCluster: \"my-cluster-source\"\ntargetCluster: \"my-cluster-target\"\nsourceConnector:\n  config:\n    replication.factor: 1\n    offset-syncs.topic.replication.factor: 1\n    sync.topic.acls.enabled: \"false\"\ntargetConnector:\n  tls:\n    trustedCertificates:\n      - secretName: my-cluster-cluster-cert\n        certificate: ca.crt\n</code></pre> </li> <li> <p>The consumer may be started in second or third step. To start it, you can use a new container or use one of the running kafka broker container. Using the <code>Docker perspective</code> in Visual Code, we can get into a bash shell within one of the Kafka broker container. The local folder is mounted to <code>/home</code>. Then the script, <code>consumeFromLocal.sh source.orders</code> will get messages from the replicated topic: <code>source.orders</code></p> </li> </ul>"},{"location":"mirrormaker/#scenario-4-from-event-streams-on-cloud-to-strimzi-cluster-on-openshift","title":"Scenario 4: From Event Streams On Cloud to Strimzi Cluster on Openshift","text":"<p>We are reusing the Event Streams on Cloud cluster on Washington DC data center as source target and the vanilla Kafka 2.4 cluster as target, also running within Washington data center in a OpenShift Cluster. As both clusters are in the same data center, we deploy Mirror Maker 2.0 close to target kafka cluster.</p> <p></p>"},{"location":"mirrormaker/#typical-errors-in-mirror-maker-2-traces","title":"Typical errors in Mirror Maker 2 traces","text":"<ul> <li>Plugin class loader for connector: 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' was not found. <ul> <li>This error message is a light issue in kafka 2.4 and does not impact the replication. In Kafka 2.5 this message is for DEBUG logs.</li> </ul> </li> <li>Error while fetching metadata with correlation id 2314 : {source.heartbeats=UNKNOWN_TOPIC_OR_PARTITION}:<ul> <li>Those messages may come from multiple reasons. One is the name topic is not created. In Event Streams topics needs to be created via CLI or User Interface. It can also being related to the fact the consumer polls on a topic that has just been created and the leader for this topic-partition is not yet available, you are in the middle of a leadership election.</li> <li>The advertised listener may not be set or found.</li> </ul> </li> <li>Exception on not being able to create Log directory: do the following: <code>export LOG_DIR=/tmp/logs</code></li> <li>ERROR WorkerSourceTask{id=MirrorSourceConnector-0} Failed to flush, timed out while waiting for producer to flush outstanding 1 messages</li> <li>ERROR WorkerSourceTask{id=MirrorSourceConnector-0} Failed to commit offsets (org.apache.kafka.connect.runtime.SourceTaskOffsetCommitter:114)</li> </ul>"},{"location":"monitoring/","title":"Monitoring Mirror Maker and kafka connect cluster","text":"<p>The goal of this note is to go over some of the details on how to monitor Mirror Maker 2.0 metrics to Prometheus and how to use Grafana dashboard.</p> <p>Prometheus is an open source systems monitoring and alerting toolkit that, with Kubernetes, is part of the Cloud Native Computing Foundation. It can monitor multiple workloads but is normally used with container workloads. </p> <p>Here is simple diagram to explain prometheus generic architecture from their main website:</p> <p></p> <p>In the context of data replication between kafka cluster, what we want to monitor the mirror maker 2.0 metrics:</p> <p></p> <p>In order to run the example Grafana dashboards to monitor MM2, you must:</p> <ol> <li>Add metrics configuration to your Kafka cluster resources (Mirror Maker 2.0)</li> <li>Deploy Prometheus and optionally Prometheus Alertmanager</li> <li>Deploy Grafana</li> </ol> <p>For the interest of monitoring we can add Kafka Exporter to assess the consumer lag.</p>"},{"location":"monitoring/#installation-and-configuration","title":"Installation and configuration","text":"<p>Prometheus deployment inside Kubernetes uses operator as defined in the coreos github. The CRDs define a set of resources. The ServiceMonitor, PodMonitor, PrometheusRule are used.</p> <p>Inside the Strimzi github repository, we can get a prometheus.yml file to deploy prometheus server. This configuration defines, ClusterRole, ServiceAccount, ClusterRoleBinding, and the Prometheus resource instance.  For your own deployment you have to change the target namespace, and the rules</p> <p>You need to deploy Prometheus and all the other elements inside the same namespace or OpenShift project as the Kafka Cluster or the KafkaConnect Cluster.</p> <p>To be able to monitor your own Kafka cluster you need to enable Prometheus metrics. An example of Kafka cluster Strimzi based deployment can be found here</p>"},{"location":"monitoring/#install-prometheus","title":"Install Prometheus","text":"<ul> <li> <p>After creating a namespace or reusing the Kafka cluster namespace, you need to deploy the Prometheus operator by first downloading the different configuration yaml files:</p> <pre><code>curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-deployment.yaml | sed -e \"s/namespace: default/namespace: jb-kafka-strimzi/\" &gt; prometheus-operator-deployment.yaml\n</code></pre> <pre><code>curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-cluster-role.yaml &gt; prometheus-operator-cluster-role.yaml\n</code></pre> <pre><code>curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-cluster-role-binding.yaml | sed -e \"s/namespace: default/namespace: jb-kafka-strimzi/\" &gt; prometheus-operator-cluster-role-binding.yaml\n</code></pre> <pre><code>curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-service-account.yaml | sed -e \"s/namespace: default/namespace: jb-kafka-strimzi/\" &gt; prometheus-operator-service-account.yaml\n</code></pre> </li> </ul> <p>Note</p> <pre><code>The prometheus-operator-deployment.yaml define security context the operator pod will use. It is set as a non root user (unprivileged). If you need to change that, or reference an existing user modify the file.\n</code></pre> <ul> <li> <p>Deploy the operator, cluster role, binding and service account (see files under <code>monitoring</code> folder):</p> <pre><code>oc apply -f prometheus-operator-deployment.yaml\noc apply -f prometheus-operator-cluster-role.yaml\noc apply -f prometheus-operator-cluster-role-binding.yaml\noc apply -f prometheus-operator-service-account.yaml\n</code></pre> </li> </ul> <p>When you apply those configuration, the following resources are managed by the Prometheus Operator:</p> Resource Description ClusterRole To grant permissions to Prometheus to read the health endpoints exposed by the Kafka and ZooKeeper pods, cAdvisor and the kubelet for container metrics. ServiceAccount For the Prometheus pods to run under. ClusterRoleBinding To bind the ClusterRole to the ServiceAccount. Deployment To manage the Prometheus Operator pod. ServiceMonitor To manage the configuration of the Prometheus pod. Prometheus To manage the configuration of the Prometheus pod. PrometheusRule To manage alerting rules for the Prometheus pod. Secret To manage additional Prometheus settings. Service To allow applications running in the cluster to connect to Prometheus (for example, Grafana using Prometheus as datasource)"},{"location":"monitoring/#deploy-prometheus","title":"Deploy prometheus","text":"<p>Note</p> <pre><code>The following section is including the configuration of a Prometheus server monitoring a full Kafka Cluster. For Mirror Maker or Kafka Connect only the configuration will have less rules, and configuration. See [next section](#mirror-maker-monitoring).\n</code></pre> <p>Deploy the prometheus server by first changing the namespace and also by adapting the original file.</p> <pre><code>curl -s  https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/prometheus.yaml | sed -e \"s/namespace: myproject/namespace: jb-kafka-strimzi/\" &gt; prometheus.yml\n</code></pre> <p>If you are using AlertManager (see section below) Define the monitoring rules of the kafka run time: KafkaRunningOutOfSpace, UnderReplicatedPartitions, AbnormalControllerState, OfflinePartitions, UnderMinIsrPartitionCount, OfflineLogDirectoryCount, ScrapeProblem (Prometheus related alert), ClusterOperatorContainerDown, KafkaBrokerContainersDown, KafkaTlsSidecarContainersDown</p> <pre><code>curl -s \nhttps://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/prometheus-rules.yaml sed -e \"s/namespace: default/namespace: jb-kafka-strimzi/\" &gt; prometheus-rules.yaml\n</code></pre> <pre><code>oc apply -f prometheus-rules.yaml\noc apply -f prometheus.yaml\n</code></pre> <p>The Prometheus server configuration uses service discovery to discover the pods in the cluster from which it gets metrics.</p>"},{"location":"monitoring/#install-grafana","title":"Install Grafana","text":"<p>Grafana provides visualizations of Prometheus metrics. Again we will use the Strimzi dashboard definition as starting point to monitor Kafka cluster but also mirror maker.</p> <ul> <li>Deploy Grafan to OpenShift and expose it via a service:</li> </ul> <pre><code>oc apply -f grafana.yaml\n</code></pre> <p>In case you want to test grafana locally run: <code>docker run -d -p 3000:3000 grafana/grafana</code></p>"},{"location":"monitoring/#kafka-explorer","title":"Kafka Explorer","text":""},{"location":"monitoring/#mirror-maker-monitoring","title":"Mirror maker monitoring","text":""},{"location":"monitoring/#configure-grafana-dashboard","title":"Configure Grafana dashboard","text":"<p>To access the Grafana portal you can use port forwarding like below or expose a route on top of the grafana service.</p> <ul> <li>Use port forwarding:</li> </ul> <pre><code>export PODNAME=$(oc get pods -l name=grafana | grep grafana | awk '{print $1}')\nkubectl port-forward $PODNAME 3000:3000\n</code></pre> <p>Point your browser to http://localhost:3000.</p> <ul> <li>Expose the route via cli</li> </ul> <p>Add the Prometheus data source with the URL of the exposed routes. http://prometheus-operated:9090</p>"},{"location":"monitoring/#alert-manager","title":"Alert Manager","text":"<p>As seen in previous section, when deploying prometheus we can set some alerting rules on elements of the Kafka cluster. Those rule examples are in the file <code>The prometheus-rules.yaml</code>. Those rules are used by the AlertManager component.</p> <p>Prometheus Alertmanager is a plugin for handling alerts and routing them to a notification service, like Slack. The Prometheus server is a client to the Alert Manager.</p> <ul> <li>Download an example of alert manager configuration file</li> </ul> <pre><code>curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/alert-manager.yaml &gt; alert-manager.yaml\n</code></pre> <ul> <li>Define a configuration for the channel to use, by starting from the following template</li> </ul> <pre><code>curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-alertmanager-config/alert-manager-config.yaml &gt; alert-manager-config.yaml\n</code></pre> <ul> <li> <p>Modify this file to reflect the remote access credential and URL to the channel server. </p> </li> <li> <p>Then deploy the secret that matches your config file .</p> </li> </ul> <pre><code>oc create secret generic alertmanager-alertmanager --from-file=alertmanager.yaml=alert-manager-config.yaml\n</code></pre>"},{"location":"python/","title":"Kafka programming with python","text":"<p>There are a lot of python based code already defined in this EDA reference project and integration tests.</p>"},{"location":"python/#basic-consumer-and-producer","title":"Basic consumer and producer","text":"<p>In the <code>python-kafka</code> folder, I used a very simple setting to run kafka locally with docker compose and python environment.</p> <p>KafkaConsumer.py is for used to connect to Kafka brokers, which URL is defined in environment variable (KAFKA_BROKERS) and uses the confluent_kafka library.</p> <p>To run this consumer using local kafka, first start kafka and zookeeper using docker compose:</p> <pre><code>cd python-kafka\ndocker-compose up &amp;\n</code></pre> <pre><code>docker rm Env1\n./startPythonDocker.sh Env1\nroot@19d06f7d163e:/home# cd python-kafka/\nroot@19d06f7d163e:/home/python-kafka# python KafkaConsumer.py\n</code></pre> <p>The producer code KafkaProducer.py is in separate program and run in a second container</p> <pre><code>docker rm Env2\n./startPythonDocker.sh Env2 5001\nroot@44e827a5c2cc:/home# cd python-kafka/\nroot@44e827a5c2cc:/home/python-kafka# python KafkaProducer.py\n</code></pre>"},{"location":"python/#using-event-streams-on-cloud","title":"Using event streams on Cloud","text":"<p>Set the KAFKA_BROKERS to the brokers URL of the event streams instance. The setenv.sh is used for that:</p> <pre><code>root@44e827a5c2cc:/home/kafka# source ./setenv.sh IBMCLOUD\necho $KAFKA_BROKERS\npython KafkaConsumer.py   \n# or \npython KafkaProducer.py\n</code></pre>"},{"location":"python/#faust","title":"Faust","text":"<p>The other API to integrate with Kafka is the Faust for streamings. To execute the first basic faust agent and producer code use the following: FaustEasiestApp.py</p> <p>As previously start a docker python container and then:</p> <pre><code>root@44e827a5c2cc:/home# pip install faust &amp; cd python-kafka\nroot@44e827a5c2cc:/home/python-kafka# faust -A FaustEasiestApp worker -l info\n</code></pre>"},{"location":"strimzi/","title":"Strimzi.io","text":"<p>Strimzi uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters. When the Strimzi Cluster Operator is up and runnning, it starts to watch for certain OpenShift or Kubernetes resources containing the desired Kafka and/or Kafka Connect cluster configuration. </p> <p></p> <p>strimzi.io/ offers the following capabilities:</p> <ul> <li>Deploy Kafka OOS on any OpenShift or k8s platform</li> <li>Support TLS and SCRAM-SHA authentication, and automated certificate management</li> <li>Operators for cluster, user and topic</li> <li>Manage kafka using gitops: See vaccine-gitops environment/strimzi folder</li> </ul>"},{"location":"strimzi/#concept-summary","title":"Concept summary","text":"<p>The Cluster Operator is a pod used to deploys and manages Apache Kafka clusters, Kafka Connect, Kafka MirrorMaker (1 and 2), Kafka Bridge, Kafka Exporter, and the Entity Operator. When deployed the following commands goes to the Cluster operator:</p> <pre><code># Get the current cluster list\noc get kafka\n# get the list of topic\noc get kafkatopics\n</code></pre>"},{"location":"strimzi/#installation-on-openshift","title":"Installation on OpenShift","text":"<p>The Strimzi operator deployment is done in two phases:</p> <ul> <li>Deploy the Custom Resource Definitions (CRDs), which act as specifications of the custom resources to deploy.</li> <li> <p>Deploy one to many instances of those CRDs</p> </li> <li> <p>Download release artifacts https://github.com/strimzi/strimzi-kafka-operator/releases.  Each CRD has a common configuration like bootstrap servers, CPU resources, logging, health checks...</p> </li> <li>Create a project: <code>oc new-project eda-strimzi-21</code></li> <li>Strimzi provides two cluster roles: <code>strimzi-view</code> and <code>strimzi-admin</code>. So to get non k8s admin user to define strimzi resource do the following: <code>oc apply -f install/strimzi-admin/</code></li> <li>Deploy the cluster operator (to watch a single namespace but it could be multiple ns). For that we need to use the namespace the Cluster Operator is going to be installed into:</li> </ul> <pre><code>sed -i '' 's/namespace: .*/namespace: eda-strimzi-21/' install/cluster-operator/*RoleBinding*.yaml\n# then deploy the operator\noc apply -f install/cluster-operator -n eda-strimzi-21\n# verify deployment and pod\noc get depoyments\n# NAME    READY UP-TO-DATE   AVAILABLE   AGE\n# strimzi-cluster-operator   1/1       1            1           116s\noc get pods\n# NAME                                        READY     STATUS    RESTARTS   AGE\n# strimzi-cluster-operator-6cb7c6c99c-9cbch   1/1       Running   0          2m3s\n</code></pre> <p>As an alternate the Strimzi git has a command that can be run with the namespace as argument</p> <pre><code>oc apply -f 'https://strimzi.io/install/latest?namespace=eda-strimzi-21' -n eda-strimzi-21\n</code></pre> <p>The commands above, should create the following service account, resource definitions, roles, and role bindings:</p> Names Resource Command strimzi-cluster-operator A service account provides an identity for processes that run in a Pod. oc get sa -l app=strimzi strimzi-cluster-operator-global, strimzi-cluster-operator-namespaced, strimzi-entity-operator, strimzi-kafka-broker, strimzi-topic-operator Cluster Roles oc get clusterrole strimzi-cluster-operator-entity-operator-delegation, strimzi-cluster-operator, strimzi-cluster-operator-topic-operator-delegation Role binding oc get rolebinding strimzi-cluster-operator, strimzi-cluster-operator-kafka-broker-delegation Cluster Role Binding oc get clusterrolebinding -l app=strimzi kafkabridges, kafkaconnectors, kafkaconnects, kafkamirrormaker2s kafka, kafkatopics, kafkausers Custom Resource Definitions oc get customresourcedefinition <p>Note</p> <p>All those resources are labelled with <code>strimzi</code> name.</p> <p>Error</p> <pre><code>In case of the installatiob fails with error like: \" kafka.kafka.strimzi.io is forbidden: User \"system:serviceaccount:eda-strimzi-21 :strimzi-cluster-operator\" cannot watch resource \"kafkas\" in API group \"kafka.strimzi.io\" in the namespace \"eda-strimzi-21 \", you need to add cluster role to the strimzi operator user by doing the following commands:\n</code></pre> <pre><code>oc adm policy add-cluster-role-to-user strimzi-cluster-operator-namespaced --serviceaccount strimzi-cluster-operator -n eda-strimzi-21\noc adm policy add-cluster-role-to-user strimzi-entity-operator --serviceaccount strimzi-cluster-operator -n eda-strimzi-21\n</code></pre> <ul> <li>Deploy Kafka Cluster using persistence or ephemeral. For dev purpose we can use ephemeral. Modify the name of the cluster in the <code>example/kafka</code> yaml file for the expected config then do one of the <code>oc apply -f examples/kafka/kafka-ephemeral.yaml</code>. The zookeeper and kafka pods should run.  Again as an alternate we can directly reference the github yaml file:</li> </ul> <pre><code>oc apply -f https://strimzi.io/examples/latest/kafka/kafka-persistent.yaml -n eda-strimzi-21 \n</code></pre> <ul> <li>Deploy Topic Operator, only if you want it to manage multiple cluster. If not then we need to add the configuration of the operator inside the Kafka Cluster yaml definition using the EntityTopicOperatorSpec schema:</li> </ul> <pre><code>entityOperator:\n  # ...\n  topicOperator:\n    watchedNamespace: eda-strimzi-21\n    reconciliationIntervalSeconds: 60\n</code></pre> <ul> <li>Deploy user operator, same logic as above, apply. A standalone deployment means the Topic Operator and User Operator can operate with a Kafka cluster that is not managed by Strimzi.</li> </ul>"},{"location":"strimzi/#validate-cluster-is-running","title":"Validate cluster is running","text":"<ul> <li>Send message</li> </ul> <pre><code>kubectl run kafka-producer -ti --image=quay.io/strimzi/kafka:0.21.1-kafka-2.7.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list eda-strimzi-kafka-bootstrap:9092 --topic test\n</code></pre> <ul> <li>Consumer message</li> </ul> <pre><code> kubectl run kafka-consumer -ti --image=quay.io/strimzi/kafka:0.21.1-kafka-2.7.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server eda-strimzi-kafka-bootstrap:9092 --topic test --from-beginning\n</code></pre>"},{"location":"strimzi/#defining-topic","title":"Defining topic","text":"<pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: kafka.strimzi.io/v1beta1\nkind: KafkaTopic\nmetadata:\n  name: transportation\n  labels:\n    strimzi.io/cluster: eda-strimzi\nspec:\n  partitions: 1\n  replicas: 1\n  config:\n    retention.ms: 7200000\n    segment.bytes: 1073741824\n\nEOF\n</code></pre>"},{"location":"strimzi/#defining-users","title":"Defining Users","text":"<p>Define a KafkaUser using yaml and then apply within the same namespace:</p> <pre><code>apiVersion: kafka.strimzi.io/v1beta1\nkind: KafkaUser\nmetadata:\n  name: tls-user\n  labels:\n    strimzi.io/cluster: vaccine-kafka\nspec:\n  authentication:\n    type: tls\n  authorization:\n    type: simple\n    acls:\n      - host: '*'\n        resource:\n          type: topic\n          name: '*'\n          patternType: literal\n        operation: Write\n      - host: '*'\n        resource:\n          type: topic\n          name: '*'\n          patternType: literal\n        operation: Read\n      - host: '*'\n        resource:\n          type: topic\n          name: '*'\n          patternType: literal\n        operation: Create\n      - host: '*'\n        resource:\n          type: group\n          name: '*'\n          patternType: literal\n        operation: Read\n</code></pre> <pre><code>oc get kafkausers\n</code></pre>"},{"location":"strimzi/#connect-client-apps","title":"Connect client apps","text":"<p>When defining the security control we want to set to access to the cluster we need to address the following questions:</p> <ul> <li> <p>Do we need applications running outside of kubernetes to access Kafka, and do we need to load balance the TCP traffic?</p> <ul> <li>For that we can use the HAProxy from Openshift via <code>Routes</code>. (Add the <code>external.type = route</code> stanza in the yaml file)</li> <li>Load balancer</li> <li>ingress with NGINX controller for kubernetes </li> </ul> </li> <li> <p>What authentication to use: TLS or SCRAM-SHA-512?</p> </li> <li>Integrate with OAuth2.0 for authentication and authorization using Open Policy agent</li> <li>Install your own certificates</li> </ul> <p>The authentications used for user and Kafka need to match. Certificates are available in PKCS #12 format (.p12) and PEM (.crt) formats.</p> <p>The internal URL name for the bootstrap can be found with: ``</p> <p>For external access we need:</p> <ul> <li></li> </ul>"},{"location":"todo/","title":"Things to cover","text":"<ul> <li>clean mirror maker 2 content between EDA and different repo</li> <li>debezium CDC to kafka</li> <li>multi tenants with kakfa quota</li> <li>mongo lab to kafka</li> <li>read from local partitioner</li> <li>apache pinot and kafka</li> <li>apache druid and kafka</li> <li>request quota on brokers- broker clinet 2.0, KIP 612, KIP 257, KIP 599</li> <li>always on and kafka </li> <li>state store with mongodb</li> <li>redis and kafka </li> <li>kafka admin managment service kmcm</li> <li>broker knows his capacities</li> </ul> <p>Be able to address * Is there defaults that we can set at cluster levels for producers and consumers?</p>"},{"location":"kconnect/","title":"Kafka Connect Labs","text":"<p>Start Kafka and Kconnect locally</p>"},{"location":"sizing/","title":"Kafka Sizing Considerations","text":"<p>For any Kafka deploy we need to compute the number of brokers, connectors and replicators needed for production, staging and dev environments.</p> <p>It is difficult to compute as Kafka has a lot of tuning factors and the needs are not easy to assess up front, during pre-sale configuration. The tee shirt size is the simplest approach in pre-sale but still some important considerations need to be evaluated.</p> <p>Generally, disk throughput tends to be the main bottleneck in Kafka performance. However, that\u2019s not to say that the network is never a bottleneck. Network throughput can affect Kafka\u2019s performance if you are sending messages across data centers, if your topics have a large number of consumers, or if your replica followers are catching up to their leaders.  Depending on how you configure flush behavior you may or may not benefit from more expensive disks (if you force flush often then higher RPM SAS drives may be better).</p> <p>But what can we still do for discussion?</p>"},{"location":"sizing/#context","title":"Context","text":"<p>As stipulated by the licensing rules we need to look at brokers, geo-replicator, Prod and DR and Kafka connector. The following figure illustrates a production deployment view with the needed components to size in the context of OpenShift or Kubernetes deployment.</p> <p></p> <ul> <li>The top region (Region-west) is for production. It includes OpenShift platform with master nodes (not represented in the diagram) and worker nodes. The worker nodes presented in the figure do concern only the Kafka brokers (9 dedicated nodes) and Kafka connect cluster nodes (3) that may be shared with other workload like kafka stream applications.</li> <li>The east region is for Disaster Recovery and will have the same amount of Kafka broker nodes, they are running as they received data from the production platform.</li> <li>The Kafka connector nodes in the DR site, are running Geo-replicator, which is based on the Mirror Maker 2 Open Source project. This component runs in cluster and will have as many nodes as needed to move to data from production topics to target topics.</li> </ul>"},{"location":"sizing/#latency-numbers-every-solution-architect-should-know","title":"Latency numbers every solution architect should know","text":"<p>Dr. Dean from Google reveals the length of typical computer operations in 2010. Some numbers are outdated as computers become faster and more powerful. However, those numbers should still be able to give us an idea of the fastness and slowness of different computer operations.</p> Operation name Time L1 cache reference 0.5 ns L2 cache reference 7 ns Mutex lock/unlock 100 ns Compress 1K bytes with Zippy 10,000 ns = 10 \u00b5s Send 2K bytes over 1 Gbps network 20,000 ns = 20 \u00b5s Read 1 MB sequentially from memory 250,000 ns = 250 \u00b5s Round trip within the same datacenter 500,000 ns = 500 \u00b5s Disk seek 10,000,000 ns = 10 ms Read 1 MB sequentially from the network 10,000,000 ns = 10 ms Read 1 MB sequentially from disk 30,000,000 ns = 30 ms Send packet CA (California) -&gt;EU -&gt;CA 150,000,000 ns = 150 ms Notes <ul> <li>ns = nanosecond, \u00b5s = microsecond, ms = millisecond</li> <li>1 ns = 10^-9 seconds</li> <li>1 \u00b5s= 10^-6 seconds = 1,000 ns</li> <li>1 ms = 10^-3 seconds = 1,000 \u00b5s = 1,000,000 ns</li> </ul>"},{"location":"sizing/#tee-shirt-sizing","title":"Tee shirt sizing","text":"<p>Once you selected the starting cluster topology it is important to continuously assess the performance of the cluster, and add capacity over time. Always consider adding enough spare capacity to withstanding an event affecting all brokers of an entire Availability Zone. </p> <p>Seimics includes a basic tee shirt cluster sizing tool. We want to update it to be more adapted to what a AWS deployment may looks like. We expect 3 availability zones, and a cluster deployed within one region for production and another one for DR. If on premise and there may no be any availability zone, so expect to get 3 racks, and so the number will be mapped to VM assign to different rack. </p> <p>See the first figure above for a typical topology.</p> <ul> <li>Consumer groups up to 10</li> </ul> Component Small Medium Large Kafka Brokers 4 x m5.xlarge ( 4 vCPU - 16GB) 9 x m5.xlarge 12 x m5.xlarge Broker VPCs 16 36 48 Kafka Connector 3 x c5.large (2 CPUs - 4 GB) 3  x c5.large 6 x c5.large KConnect VPCs 6 6 12 DR Kafka Brokers 4 x m5.xlarge ( 4 vCPU - 16G) 9 x m5.xlarge 12 x m5.xlarge Broker VPCs 16 36 48 DR Kafka Connector 6 6 12 KConnect VPCs 16 36 48 DR Geo replication 3 c5.large (2 CPUs - 4 GB) 3  x c5.large 6  x c5.large Geo VPCs 6 6 12 Zookeepers - not license 3 c5.large (2 CPUs - 4 GB) 5 5 Grant Total VPCs 50 90 132 <p>With m5.xlarge the expected max network throughput is:</p> <p>To this nodes you need to add EBS and master nodes to the OpenShift cluster.</p>"},{"location":"sizing/#customer-assessment","title":"Customer Assessment","text":"<p>We have enought concepts to discuss with architects. We need to know the following information for their target IT infrastructure. For that we sould try to work backward from their use case to determine the throughput, availability, durability, and latency requirements. Here are a set of questions to assess</p> <ul> <li>Are you planning to run on cloud provider or on your own hardware on-premise? If so do you have a characteristics of your hardware.</li> </ul> Impacts <p>As seen previously, disk, network and machine characteristics are key factor. You can use the formula of total expected cluster throughput to present the impact on those hardware characteristics. With cloud providers there are for sure a lot of different VM profile, disk storage and network capacity. It is important to assess what the customer is actually provisioning.</p> <ul> <li>Number of producer applications and their expected throughput in Megabyte per second - peak and average time ?</li> </ul> Impacts <p>This is major parameters for cluster sizing, as explained in the next section, the total throughput, impact performance, and disk sizing.</p> <ul> <li>What peak traffic really means ? It is important to assess how long is the peak, and how more data are sent.</li> <li>How many hours per day / days per week the traffic is sustained?</li> <li>Expected retention time ?  The longer the retention time, the more disk space is needed.</li> <li>Number of different consumer types (consumer group) ?</li> </ul> Impacts <p>Bigger number of consumer groups will lead Kafka brokers to do a lot of processing, impacting the size of the memory and number of CPUs needed.</p> <ul> <li>Number of streaming apps?: this could be difficult to assess but a streaming app most likely will add topics, which may not have been seen by the customer.</li> <li>Cloud provider</li> </ul> Impacts <p>This is mostly to assess the capability to select powerful virtual machine, storage area network or be able to connect disks directly to VMs.</p> <ul> <li>Do we need encrypted communication between apps in the same cluster: Turning on in-cluster encryption forces the brokers to encrypt and decrypt individual messages which results in additional CPU and memory bandwidth overhead.</li> </ul> Impacts <p>Encrypting means using bigger machine for bigger CPUs. </p> <p>Assumptions you can share:</p> <ul> <li>each producer will have at least one topic</li> <li>each streaming will add one or more topic</li> <li>kafka connector add topics</li> <li>topics could have two strategies: delete or compact - It will be difficult for an architect to answer what they will use. Knowing that they will have both.</li> <li>replicas is defaulted to 3 and in-sync replicas to 2</li> </ul> <p>Going into details will kill any simple estimations:</p> <ul> <li>retention time set to multiple days may not be necessary for all topics.</li> <li>each time we add a streaming application, we add a topic and we add files to volume.</li> <li>compression could be used to reduce payload size</li> <li>Each topic may have different retention, partition numbers, strategy...</li> <li>The I/O throughput is impacted by caching strategy...</li> </ul>"},{"location":"sizing/#deeper-dive-discussion","title":"Deeper dive discussion","text":""},{"location":"sizing/#kafka-brokers","title":"Kafka Brokers","text":""},{"location":"sizing/#concepts","title":"Concepts","text":"<p>It is important to communicate clearly on the different elements that help to size a Kafka cluster. The following diagram presents all the needed concepts.</p> <p></p> <ul> <li>On the left you will have different producer applications: any applications running Kafka Producer API, and Kafka source connectors.  Each of those application sends data which adds to the total throughput.</li> <li>The producer applications may be a change data capture agent in Kafka Connect (e.g. Debezium), event-driven microservice apps or any Kafka source connectors. The list of producers is not exhaustive, this is used for illustration purpose.</li> <li>Producer's throughput is impacted by a lot of parameters, one of the most important is the level of write acknowledge expected. If producer apps expect all replicas to be completed (ack=-1), to ensure no data loss, the replica and in-sync replicas settings, for each topics, are becoming important. </li> <li>The diagram above, illustrates the minimum cluster size for high availability to support 2 failure/network isolations. Normally at least 4 nodes are recommended, as during maintenance we always want to have 3 nodes active to ensure a good availability. </li> <li>In the figure, each horizontal cylinders are topics / partitions. For example the red topic has 3 partitions.</li> <li>Each partition has replicas, and one broker is the partition leader (cylinder in standard color, while replicas are in sketch mode)</li> <li>As partitions are persisted as append logs on disk, they are different factors that affect performance: the storage bandwidth of the node on which Kafka Broker runs, and the performance of the disks attached to the node. Also to be efficient, the partition to disk need to be well balanced. The storage bandwidth needs to be well known as adding more disk on a saturated bandwidth will not help.</li> </ul> <p></p> <ul> <li>With Cloud deployment, like AWS, the storage attached to an instance (EC2) is based on network block storage (EBS) so network throughput is also part of the equation. You may use a dedicated network between the EC2 instance and the EBS servers to isolate IO traffic. Each storage has limit on the size you can provision, therefore it is possible to run out of disk space. </li> <li>The following figure illustrates the producer, broker operations in solution with producers sending records evenly to 3 partitions, with replica set to 3, and consumers reading from the last offset:</li> </ul> <p></p> <ul> <li>Each partition is save in a unique disk, so if a partition increase in a disk, the node can run out of disk space even if there is space to other attached volumes.</li> <li>The number of consumer group impacts the broker throughput, as broker cpu will be used to manage the consumer group and partition allocation.</li> <li>The global throughput of your cluster will be the minimum between different elements as illustrated by the following formula:</li> </ul> <p>$max_cluster_throughput = min($ $max(storage_throughput) * nb_brokers / replicas_factor,$ $max(san_network_throughput) * nb_brokers / replicas_factor,$  $max(network_throughput_ * nb_brokers / (nb_consumer _groups + replicas_factor-1))$</p> <ul> <li>In fact we should cap this number with a maximum percent like 80-85% of the max potential cluster throughput. In reality the cluster will process less than this theorical limit because producer may not send record well balanced and consumers can come and go.</li> <li>Kafka was optimized to build append log, so good sequential read. Also as it uses fsynch to flush the memory to disk via OS capability write is also sequential, which increase throughput. Which also means it is not limited by disk IOPS. Consumer apps reading from the last offset will read from the memory improving throughput too. But consumers that are slow will impact the overall cluster throughput as the broker will have to load data from disk, and page the records, impact existing cache. The following figure illustrates those concepts:</li> </ul> <p></p> <ul> <li>As illustrated in previous figure, the throughput of the storage backend depends on the data that producers are sending directly to the broker plus the replication traffic the broker is receiving from its peers.</li> </ul> <p>$$ storage_throughput &lt;= input_throughput * nb_brokers / replicas_factor $$</p> <ul> <li> <p>Finally as we can see a 3 broker cluster will have the real potential throughput of only one broker, as other brokers are doing partition replications. Which means if we keep 3 replicas and 2 ISR then to scale above the IOPS of one broker, we need more disk per broker and then more brokers.</p> </li> <li> <p>The number of io thread to write to different disk are also limited by the number of CPUs. Below is an extract from the Event Streams cluster custom definition based on strimzi:</p> </li> </ul> <pre><code>spec:\n  strimziOverrides:\n    kafka:\n      config:\n        log.cleaner.threads: 6\n        num.io.threads: 8\n        num.network.threads: 12\n        num.replica.fetchers: 3\n        offsets.topic.replication.factor: 3\n        default.replication.factor: 3\n        min.insync.replicas: 2\n</code></pre> <ul> <li>Streaming applications consume messages, process them and to produce in new topic: most of the time prospect has not think about those new topics.</li> <li>Event Streams brokers run in Worker node in Kubernetes cluster. We can assume a 60-70% CPU utilization</li> <li>The broker runs as pod, and get their volume via PVC, Persistence Volume and storage class. </li> <li>As brokers are pods, they will have cpu/memory request and limit resources contraint to behave well in kubernetes cluster. Here is an example of such resource constraint, but it is better to understand what is the underlying node capabilities. </li> </ul> <pre><code>    resources: \n      requests:\n        memory: 8Gi\n        cpu: \"4\"\n      limits:\n        memory: 48Gi\n        cpu: \"8\"\n</code></pre> <ul> <li> <p>For Kafka, the following aspects of a deployment can impact the resources you need:</p> <ul> <li>Expected total number of message per second and size of messages</li> <li>The number of network threads handling messages (in the cluster configuration)</li> <li>The number of io threads handling the write operations (in the cluster configuration)</li> <li>The number of CPUs per node.</li> <li>Buffer flush policies</li> <li>The number of producers and consumers</li> <li>The number of topics and partitions</li> </ul> </li> </ul> <p>As an example on cluster with 8 CPU, 32Gb node, we can have 16 threads for the io and 32 for the network.</p> <ul> <li> <p>Kafka runs inside a JVM and uses an operating system page cache to store message data before writing to disk. The memory request for Kafka should fit the JVM heap and page cache.</p> </li> <li> <p>Worker node will run on physical node with multiple cores - Assumes 12 cores 48GB RAM. Remember that having bigger node will have a bigger impact to the overall cluster performance in case of failure, as more resources need to be re-allocated. On the other side, when using large node on cloud provider like AWS, we get better IO SAN (EBS) throughput, and event higher network throughput.</p> </li> <li>In Kubernetes, each node will have a set of PVs mounted to it - We can try to limit one disk per core, but most of the time it goes less than that. </li> <li>Let assume 2 TB per volume.</li> <li> <p>On a node we may have 6 TB of disk, let say the max capacity will be at 85% so we have a capacity per node of 10.2 TB (1TB * .85 * 12).</p> </li> <li> <p>With OpenShift or Kubernetes the recommendation is to use block storage and technology like IBM Spectrum, Ceph, or OCS and configure the Broker to use JBOD</p> </li> </ul> <p>Recall that every broker is constrained by a particular resource (typically I/O) adding more partitions will not result in increased throughput. Instead, you need to add brokers to the cluster. So sizing will be linked to the number of disk, replicas, number of consumer groups and number of broker. With 3 brokers and 3 replicas and one disk attach per brokers the amount of data will be only 85% of \u2153 of the volume size, and throughput \u2153 of the IOPS of the disk. This is why the minimum should be 6 brokers.</p> <p>There are other considerations when looking at the consumer profile:</p> <ul> <li>consumers that connect later to process older records, may lead the broker to load data from disk as most likely those records are not more in memory. The access will be non sequential read, which impact performance.</li> </ul>"},{"location":"sizing/#hardware-considerations","title":"Hardware considerations","text":"<p>With OpenShift or Kubernetes deployment a lot of things need to be addressed. First we assume we deploy in one region with at least three availability zones. The figure below illustrates a AWS deployment. The Master nodes are not represented, as Event Streams / Kafka brokers run in worker nodes. A dedicated network / subnet manage the traffic between EC2 instances. The diagram illustrates 9 brokers running as worker nodes on EC2 type (m5.4xlarge).</p> <p></p> <p>There are two ways to configure storage, using instance disks, and storage area network. The EC2 I class, is for instance storage and it achieves higher IOPS and disk throughput.</p> <p></p> <p>Here is an example of EC2 instance, oriented I/O (I3), with large capacity, which can be considered for production deployment:</p> <p></p> <p>It is important to note that in case of node failure, starting a new node, will lead to broker replication, which is part of the Kafka design, but still cost time to converge.</p> <p>For SAN deployment, in AWS, EBS storage mounted to the EC2 instances. As the storage is externalized from the EC2 instance, in case of failure and recreation, the volume can be attached and the broker starts with less replication to complete.</p> <p></p> <p>Some EBS volume types, such as gp3, io2, and st1, also allow you to adapt the throughput and IOPS. With EBS the cluster throughput will be</p> <p>$max_cluster_throughput = min($ $max(storage_throughput) * nb_brokers / replicas_factor,$ $max(san_network_throughput) * nb_brokers / replicas_factor)$</p> <p>Other SAN is to use dedicated kubernetes worker nodes using the <code>rook</code> operator for k8s and <code>ceph</code> cluster:</p> <p></p> <p>Rook is a cloud native storage orchestrator, which abstract the management and monitoring of storage cluster. Rook turns storage software into self-managing, self-scaling, and self-healing storage services. It uses operators. Ceph is a highly scalable distributed storage solution for block storage, object storage, and shared filesystems. It uses its own replication mechanism, therefore will impact the overall sizing of Kafka cluster. It is not recommended to use such storage layer. </p> <p>The figure above illustrates the rook and ceph approach.</p> <ul> <li>Replica on a disk has data but also index file See Kafka's doc on persistence which means the total size of the disk is 1.3 time greater than expected message volume.</li> <li>Need to compute the disk size requirement per partition. But if customer communicates on total throughput, partitioning will split this amount. Still consider overhead of indexing per partition.</li> </ul> <p>For better performance and resilience, Strimzi recommends to use JBOD storage., so the configuration of the kafka broker looks like:</p> <pre><code>    kafka:\n      replicas: 9\n      storage:\n        type: jbod\n        volumes:\n        - id: 0  \n          class: localblock-sc\n          size: 2048Gi\n          type: persistent-claim\n        - id: 1 \n          class: localblock-sc\n          size: 2048Gi\n          type: persistent-claim\n</code></pre> <p>Kafka team recommends using multiple drives to get good throughput and not sharing the same drives used for Kafka data with application logs or other OS filesystem activity to ensure good latency.</p> <p>Kafka always immediately writes all data to the filesystem (<code>/var/lib/kafka/data/kafka-log_1</code>) and supports the ability to configure the flush policy that controls when data is forced out of the OS cache and onto disk using the flush. With the JBOD, volumes will be used by the Kafka brokers as log directories mounted into the following path: <code>/var/lib/kafka/data-id/kafka-log_idx_</code></p> <p>Note that durability in Kafka does not require syncing data to disk, as a failed node will always recover from its replicas.</p> <p>From AWS machine type can assume the EBS volume has a baseline throughput of 250 MB/sec from standard config and 1000 MBps for gp3 (SSD), io2 (provisioned IOPS SSD), or st1 (throughput optimized HDD) volume types. This means if we need more than a total thourghput of 200 MB/s (80% of 250) we need to add brokers to the cluster. Performance reports show a six-node cluster has almost double the throughput of the three-node cluster. Larger brokers have a higher network baseline throughput (up to 25 Gb/sec) and can therefore support more consumer groups reading from the cluster. There is no way to increase network throughput other than scaling up.</p> <p>Note</p> <p>So as a conclusion we do not recommend to use ceph for Kafka volume, and attach directly disk to the machines used for brokers deployment or use EC2 optimized for EBS volume with good IOPS.  With strong elastic capacity like in AWS, it is relevant to select provisioned throughput using volume type io2.</p> <p>For IBM storage class explanation and quality of service see storage block note  and the storage class QOS</p>"},{"location":"sizing/#benchmark-your-disk","title":"Benchmark your disk","text":"<p>The linux tool to test IOPS is fio (the Flexible IO Tester). In Kubernetes we can use an image and declare a PVC on the same storage class as Kafka cluster will use, and then run tests. We have defined such configuration in our gitops repositories:</p> <ul> <li>Update the diskbench.yaml file from the real time inventory gitops with the storage class you are using for Event Streams.</li> </ul> <p>Be sure to test multiple disk sizes as most cloud providers price IOPS per GB provisioned. So a 4000Gi volume will perform better than a 1000Gi volume. </p> <ul> <li>Claim the PVC and start the disk bench job with:</li> </ul> <pre><code>oc apply -f diskbench.yaml\n</code></pre> <ul> <li>The Job runs a series of <code>fio</code> tests on the newly provisioned disk, currently there are 9 tests, 15s per test - total runtime is around 2.5 minutes</li> </ul> <p>See the Docker image for fio tool in dockerhub for information on this container.</p> <ul> <li>Look at report:</li> </ul> <pre><code>oc logs -f job/dbench\n</code></pre> <p>Below is an example of <code>fio</code> report for tests on a ceph mounted disk on one of the CoC cluster:</p> <pre><code># ... almost 100Mib/s of throughput, 1480 MB read during 15s - default block size is 4k\nread: IOPS=25.3k, BW=98.7MiB/s (103MB/s)(1480MiB/15003msec)\n# write almost 13Mb/s\nwrite: IOPS=3249, BW=12.8MiB/s (13.4MB/s)(191MiB/15056msec)\nWRITE: bw=12.8MiB/s (13.4MB/s), 12.8MiB/s-12.8MiB/s (13.4MB/s-13.4MB/s), io=191MiB (201MB), run=15056-15056msec\n# read bandwidth\nread: IOPS=2667, BW=334MiB/s (350MB/s)(5021MiB/15037msec)\n# write\nwrite: IOPS=528, BW=66.6MiB/s (69.8MB/s)(1004MiB/15090msec)\n# Testing Read Latency...\nread: IOPS=3820, BW=14.1MiB/s (15.7MB/s)(224MiB/15002msec)\n# Write latency\nwrite: IOPS=1291, BW=5166KiB/s (5290kB/s)(75.8MiB/15006msec)\n# Testing Read/Write Mixed...  (may be sloser to what kafka brokers do)\nread: IOPS=7489, BW=29.3MiB/s (30.7MB/s)(439MiB/15008msec)\nwrite: IOPS=2516, BW=9.86MiB/s (10.4MB/s)(148MiB/15008msec)\n\n==================\n= Dbench Summary =\n==================\nRandom Read/Write IOPS: 25.3k/3249. BW: 334MiB/s / 66.6MiB/s\nAverage Latency (usec) Read/Write: 1043.52/\nSequential Read/Write: 359MiB/s / 107MiB/s\nMixed Random Read/Write IOPS: 7489/2516\n</code></pre> Interpret fio results <ul> <li>See this blog for extended detail, but to summarize:</li> <li>sequential read/write are important  </li> </ul> <ul> <li>Be sure to clean your mess</li> </ul> <pre><code>oc delete -f diskbench.yaml\n</code></pre>"},{"location":"sizing/#kafka-performance-testing","title":"Kafka performance testing","text":"<p>Let start by the configuration to study. Assess the Kafka Broker server.properties content which is under the <code>/opt/kafka/config</code> folder. </p> <pre><code># The number of threads that the server uses for receiving requests from the network and sending responses to the network\nnum.network.threads=3\n\n# The number of threads that the server uses for processing requests, which may include disk I/O\nnum.io.threads=8\n# Where data is persisted\nlog.dirs=/tmp/kafka-logs\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir=1\n</code></pre> <p>Consider looking at configurations which control the flush of data to disk. There are a few important trade-offs to consider:</p> <ul> <li>Durability: Unflushed data may be lost if you are not using replication.</li> <li>Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.</li> <li>Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.</li> </ul> <p>The settings defines the flush policy to flush data after a period of time or every N messages (or both). This can be done globally and overridden on a per-topic basis</p> <pre><code># The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages=10000\n# The maximum amount of time a message can sit in a log before we force a flush\nlog.flush.interval.ms=1000\n</code></pre> <ul> <li>Create a eda-perf-topic with 5 partitions and 3 replicas (example in )</li> </ul> <pre><code>oc apply -f eda-perf-topic.yaml\n</code></pre> <p>Use internal producer perf test tool:</p> <pre><code>./kafka-producer-perf-test.sh --topic eda-perf-test --num-records 20000 --record-size 1000 --print-metrics --throughput 300 --producer-props bootstrap.servers=dev-kafka-bootstrap.rt-inventory-dev.svc:9092\n</code></pre> <ul> <li>interesting metrics to look at:</li> </ul> <pre><code>producer-metrics:connection-count:{client-id=perf-producer-client}                              : 4.000\n\nproducer-metrics:incoming-byte-rate:{client-id=perf-producer-client}                            : 18259.896\nproducer-metrics:incoming-byte-total:{client-id=perf-producer-client}                           : 1192836.000\n\nproducer-metrics:io-ratio:{client-id=perf-producer-client}                                      : 0.042\nproducer-metrics:io-time-ns-avg:{client-id=perf-producer-client}                                : 51450.563\nproducer-metrics:io-time-ns-total:{client-id=perf-producer-client}                              : 3055959438.000\nproducer-metrics:io-wait-ratio:{client-id=perf-producer-client}                                 : 0.919\nproducer-metrics:io-wait-time-ns-avg:{client-id=perf-producer-client}                           : 1122985.600\nproducer-metrics:io-wait-time-ns-total:{client-id=perf-producer-client}                         : 59169150670.000\nproducer-metrics:io-waittime-total:{client-id=perf-producer-client}                             : 59169150670.000\n\nproducer-metrics:network-io-rate:{client-id=perf-producer-client}                               : 570.384\nproducer-metrics:network-io-total:{client-id=perf-producer-client}                              : 37198.000\n\nproducer-metrics:record-queue-time-avg:{client-id=perf-producer-client}                         : 0.095\nproducer-metrics:record-queue-time-max:{client-id=perf-producer-client}                         : 7.000\n# record throughtput was set to 300 so simulator was close\nproducer-metrics:record-send-rate:{client-id=perf-producer-client}                              : 299.867\nproducer-metrics:record-send-total:{client-id=perf-producer-client}                             : 20000.000\nproducer-metrics:record-size-avg:{client-id=perf-producer-client}                               : 1086.000\nproducer-metrics:record-size-max:{client-id=perf-producer-client}                               : 1086.000\n# Latency cross nodes, then with at node level\nproducer-metrics:request-latency-avg:{client-id=perf-producer-client}                           : 3.499\nproducer-metrics:request-latency-max:{client-id=perf-producer-client}                           : 38.000\nproducer-node-metrics:request-latency-avg:{client-id=perf-producer-client, node-id=node-0}      : 4.091\nproducer-node-metrics:request-latency-avg:{client-id=perf-producer-client, node-id=node-1}      : 3.105\nproducer-node-metrics:request-latency-avg:{client-id=perf-producer-client, node-id=node-2}      : 3.589\nproducer-node-metrics:request-latency-max:{client-id=perf-producer-client, node-id=node-0}      : 20.000\nproducer-node-metrics:request-latency-max:{client-id=perf-producer-client, node-id=node-1}      : 18.000\nproducer-node-metrics:request-latency-max:{client-id=perf-producer-client, node-id=node-2}      : 38.000\n\nproducer-metrics:request-rate:{client-id=perf-producer-client}                                  : 285.192\nproducer-metrics:request-size-avg:{client-id=perf-producer-client}                              : 1188.969\nproducer-metrics:request-size-max:{client-id=perf-producer-client}                              : 2214.000\nproducer-node-metrics:request-rate:{client-id=perf-producer-client, node-id=node-0}             : 58.021\nproducer-node-metrics:request-rate:{client-id=perf-producer-client, node-id=node-1}             : 113.459\nproducer-node-metrics:request-rate:{client-id=perf-producer-client, node-id=node-2}             : 113.795\n\nproducer-node-metrics:response-rate:{client-id=perf-producer-client, node-id=node-0}            : 58.027\nproducer-node-metrics:response-rate:{client-id=perf-producer-client, node-id=node-1}            : 113.411\nproducer-node-metrics:response-rate:{client-id=perf-producer-client, node-id=node-2}            : 113.805\nproducer-topic-metrics:byte-rate:{client-id=perf-producer-client, topic=eda-perf-test}          : 319971.654\n</code></pre> Read more <ul> <li>Kafka doc on hardware and OS need</li> <li>Datadog article: Monitoring Kafka performance metrics</li> </ul> More Reading <ul> <li>Strimzi configuration</li> <li>Kafka performance test tool</li> <li>AWS Best practices for right-sizing your Apache Kafka clusters to optimize performance and cost</li> <li>Our testing tool</li> <li>AWS perf testing framework for Kafka</li> <li>For IBM storage class explanation </li> </ul> <p>and the storage class QOS</p>"}]}